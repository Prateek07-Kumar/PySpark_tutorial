{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "tTUEcPQ-Oh6V",
        "outputId": "ba3c5187-0b0e-492a-e7c8-f5aeb8a968f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x79f728bc8650>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://cc08b8ac5fd0:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Optimizing Skewness and Spillage</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .appName(\"Optimizing Skewness and Spillage\")\n",
        "    # .master(\"spark://192.168.1.12:7077\")\n",
        "    .master(\"local[*]\")  # Use local mode for development\n",
        "    .config(\"spark.cores.max\", 8)\n",
        "    .config(\"spark.executor.cores\", 4)\n",
        "    .config(\"spark.executor.memory\", \"512M\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u-H0PNEJOh6Y"
      },
      "outputs": [],
      "source": [
        "# Disable AQE and Broadcast join\n",
        "\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
        "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "W2YFnUdSOh6Y"
      },
      "outputs": [],
      "source": [
        "# Read EMP CSV data\n",
        "\n",
        "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
        "\n",
        "emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(r\"/content/employee_records.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uI77GZO1Oh6Y"
      },
      "outputs": [],
      "source": [
        "# Read DEPT CSV data\n",
        "\n",
        "_dept_schema = \"department_id int, department_name string, description string, city string, state string, country string\"\n",
        "\n",
        "dept = spark.read.format(\"csv\").schema(_dept_schema).option(\"header\", True).load(r\"/content/department_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P68iapfeOh6Z"
      },
      "outputs": [],
      "source": [
        "# Join Datasets\n",
        "\n",
        "df_joined = emp.join(dept, on=emp.department_id==dept.department_id, how=\"left_outer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SOa3V8sSOh6Z"
      },
      "outputs": [],
      "source": [
        "df_joined.write.format(\"noop\").mode(\"overwrite\").save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6kJ61mEOh6Z",
        "outputId": "9cb751be-3532-4688-ade0-323aca841f3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "*(4) SortMergeJoin [department_id#7], [department_id#16], LeftOuter\n",
            ":- *(1) Sort [department_id#7 ASC NULLS FIRST], false, 0\n",
            ":  +- Exchange hashpartitioning(department_id#7, 200), ENSURE_REQUIREMENTS, [plan_id=70]\n",
            ":     +- FileScan csv [first_name#0,last_name#1,job_title#2,dob#3,email#4,phone#5,salary#6,department_id#7] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/employee_records.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<first_name:string,last_name:string,job_title:string,dob:string,email:string,phone:string,s...\n",
            "+- *(3) Sort [department_id#16 ASC NULLS FIRST], false, 0\n",
            "   +- Exchange hashpartitioning(department_id#16, 200), ENSURE_REQUIREMENTS, [plan_id=82]\n",
            "      +- *(2) Filter isnotnull(department_id#16)\n",
            "         +- FileScan csv [department_id#16,department_name#17,description#18,city#19,state#20,country#21] Batched: false, DataFilters: [isnotnull(department_id#16)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/department_data.csv], PartitionFilters: [], PushedFilters: [IsNotNull(department_id)], ReadSchema: struct<department_id:int,department_name:string,description:string,city:string,state:string,count...\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Explain Plan\n",
        "\n",
        "df_joined.explain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWQanVRROh6Z",
        "outputId": "91b0cc7a-dd27-46d9-edb9-228a83cb35e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------+\n",
            "|partition_num| count|\n",
            "+-------------+------+\n",
            "|           42|473632|\n",
            "+-------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check the partition details to understand distribution\n",
        "from pyspark.sql.functions import spark_partition_id, count, lit\n",
        "\n",
        "part_df = df_joined.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\").agg(count(lit(1)).alias(\"count\"))\n",
        "\n",
        "part_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jm49c9LOh6a",
        "outputId": "f457748f-fb6c-460b-9642-3126cea64976"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------+\n",
            "|department_id|count(1)|\n",
            "+-------------+--------+\n",
            "|         NULL|  473632|\n",
            "+-------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Verify Employee data based on department_id\n",
        "from pyspark.sql.functions import count, lit, desc, col\n",
        "\n",
        "emp.groupBy(\"department_id\").agg(count(lit(1))).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "S5hM9lshOh6a"
      },
      "outputs": [],
      "source": [
        "# Set shuffle partitions to a lesser number - 16\n",
        "\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htWfXPP2Oh6a",
        "outputId": "58c547b4-3fb8-4a8b-c005-c96acb5a5e84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "|  5|\n",
            "|  6|\n",
            "|  7|\n",
            "|  8|\n",
            "|  9|\n",
            "| 10|\n",
            "| 11|\n",
            "| 12|\n",
            "| 13|\n",
            "| 14|\n",
            "| 15|\n",
            "| 16|\n",
            "| 17|\n",
            "| 18|\n",
            "| 19|\n",
            "+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Let prepare the salt\n",
        "import random\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "# UDF to return a random number every time and add to Employee as salt\n",
        "@udf\n",
        "def salt_udf():\n",
        "    return random.randint(0, 32)\n",
        "\n",
        "# Salt Data Frame to add to department\n",
        "salt_df = spark.range(0, 32)\n",
        "salt_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt0gWprqOh6a",
        "outputId": "20bac466-67c8-4fb7-8023-bcbe54d3ed41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+--------------+\n",
            "|first_name| last_name|           job_title|       dob|               email|               phone|  salary|department_id|salted_dept_id|\n",
            "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+--------------+\n",
            "|   Richard|  Morrison|Public relations ...|1973-05-05|melissagarcia@exa...|       (699)525-4827|512653.0|         NULL|          NULL|\n",
            "|     Bobby|  Mccarthy|   Barrister's clerk|1974-04-25|   llara@example.net|  (750)846-1602x7458|999836.0|         NULL|          NULL|\n",
            "|    Dennis|    Norman|Land/geomatics su...|1990-06-24| jturner@example.net|    873.820.0518x825|131900.0|         NULL|          NULL|\n",
            "|      John|    Monroe|        Retail buyer|1968-06-16|  erik33@example.net|    820-813-0557x624|485506.0|         NULL|          NULL|\n",
            "|  Michelle|   Elliott|      Air cabin crew|1975-03-31|tiffanyjohnston@e...|       (705)900-5337|604738.0|         NULL|          NULL|\n",
            "|    Ashley|   Montoya|        Cartographer|1976-01-16|patrickalexandra@...|        211.440.5466|483339.0|         NULL|          NULL|\n",
            "| Nathaniel|     Smith|     Quality manager|1985-06-28|  lori44@example.net|        936-403-3179|419644.0|         NULL|          NULL|\n",
            "|     Faith|  Cummings|Industrial/produc...|1978-07-01| ygordon@example.org|       (889)246-5588|205939.0|         NULL|          NULL|\n",
            "|  Margaret|    Sutton|Administrator, ed...|1975-08-16| diana44@example.net|001-647-530-5036x...|671167.0|         NULL|          NULL|\n",
            "|      Mary|    Sutton|   Freight forwarder|1979-12-28|  ryan36@example.com|   422.562.7254x3159|993829.0|         NULL|          NULL|\n",
            "|      Jake|      King|       Lexicographer|1994-07-11|monica93@example.org|+1-535-652-9715x6...|702101.0|         NULL|          NULL|\n",
            "|   Heather|     Haley|         Music tutor|1981-06-01|stephanie65@examp...|   (652)815-7973x298|570960.0|         NULL|          NULL|\n",
            "|    Thomas|    Thomas|Chartered managem...|2001-07-17|pwilliams@example...|001-245-848-0028x...|339441.0|         NULL|          NULL|\n",
            "|   Leonard|   Carlson|       Art therapist|1990-10-18|gabrielmurray@exa...|          9247590563|469728.0|         NULL|          NULL|\n",
            "|      Mark|      Wood|   Market researcher|1963-10-13|nicholas76@exampl...|   311.439.1606x3342|582291.0|         NULL|          NULL|\n",
            "|    Tracey|Washington|Travel agency man...|1986-05-07|  mark07@example.com|    001-912-206-6456|146456.0|         NULL|          NULL|\n",
            "|   Rachael| Rodriguez|         Media buyer|1966-12-02|griffinmary@examp...| +1-791-344-7586x548|544732.0|         NULL|          NULL|\n",
            "|      Tara|       Liu|   Financial adviser|1998-10-12|alexandraobrien@e...|        216.696.6061|399503.0|         NULL|          NULL|\n",
            "|       Ana|    Joseph|      Retail manager|1995-01-10|  rmorse@example.org|  (726)363-7526x9965|761988.0|         NULL|          NULL|\n",
            "|   Richard|      Hall|Engineer, civil (...|1967-03-02|brandoncardenas@e...| (964)451-9007x22496|660659.0|         NULL|          NULL|\n",
            "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Salted Employee\n",
        "from pyspark.sql.functions import lit, concat\n",
        "\n",
        "salted_emp = emp.withColumn(\"salted_dept_id\", concat(\"department_id\", lit(\"_\"), salt_udf()))\n",
        "\n",
        "salted_emp.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Salted Department\n",
        "\n",
        "salted_dept = dept.join(salt_df, how=\"cross\").withColumn(\"salted_dept_id\", concat(\"department_id\", lit(\"_\"), \"id\"))\n",
        "\n",
        "salted_dept.where(\"department_id = 9\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RZXGTZtW9dL",
        "outputId": "6682107f-82ee-44c8-ad29-2e0eb92bee2d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------------------+--------------------+-----------+-----+-------+---+--------------+\n",
            "|department_id|     department_name|         description|       city|state|country| id|salted_dept_id|\n",
            "+-------------+--------------------+--------------------+-----------+-----+-------+---+--------------+\n",
            "|            9|Mcmahon, Terrell ...|De-engineered hig...|Marychester|   MN|  Italy|  0|           9_0|\n",
            "|            9|Mcmahon, Terrell ...|De-engineered hig...|Marychester|   MN|  Italy|  1|           9_1|\n",
            "|            9|Mcmahon, Terrell ...|De-engineered hig...|Marychester|   MN|  Italy|  2|           9_2|\n",
            "|            9|Mcmahon, Terrell ...|De-engineered hig...|Marychester|   MN|  Italy|  3|           9_3|\n",
            "|            9|Mcmahon, Terrell ...|De-engineered hig...|Marychester|   MN|  Italy|  4|           9_4|\n",
            "|            9|Mcmahon, Terrell ...|De-engineered hig...|Marychester|   MN|  Italy|  5|           9_5|\n",
            "|            9|Mcmahon, Terrell ...|De-engineered hig...|Marychester|   MN|  Italy|  6|           9_6|\n",
            "|            9|Mcmahon, Terrell ...|De-engineered hig...|Marychester|   MN|  Italy|  7|           9_7|\n",
            "|            9|Mcmahon, Terrell ...|De-engineered hig...|Marychester|   MN|  Italy|  8|           9_8|\n",
            "|            9|Mcmahon, Terrell ...|De-engineered hig...|Marychester|   MN|  Italy|  9|           9_9|\n",
            "|            9|Mcmahon, Terrell ...|De-engineered hig...|Marychester|   MN|  Italy| 10|          9_10|\n",
            "|            9|Mcmahon, Terrell ...|De-engineered hig...|Marychester|   MN|  Italy| 11|          9_11|\n",
            "|            9|Mcmahon, Terrell ...|De-engineered hig...|Marychester|   MN|  Italy| 12|          9_12|\n",
            "|            9|Mcmahon, Terrell ...|De-engineered hig...|Marychester|   MN|  Italy| 13|          9_13|\n",
            "|            9|Mcmahon, Terrell ...|De-engineered hig...|Marychester|   MN|  Italy| 14|          9_14|\n",
            "|            9|Mcmahon, Terrell ...|De-engineered hig...|Marychester|   MN|  Italy| 15|          9_15|\n",
            "|            9|Mcmahon, Terrell ...|De-engineered hig...|Marychester|   MN|  Italy| 16|          9_16|\n",
            "|            9|Mcmahon, Terrell ...|De-engineered hig...|Marychester|   MN|  Italy| 17|          9_17|\n",
            "|            9|Mcmahon, Terrell ...|De-engineered hig...|Marychester|   MN|  Italy| 18|          9_18|\n",
            "|            9|Mcmahon, Terrell ...|De-engineered hig...|Marychester|   MN|  Italy| 19|          9_19|\n",
            "+-------------+--------------------+--------------------+-----------+-----+-------+---+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets make the salted join now\n",
        "\n",
        "salted_joined_df = salted_emp.join(salted_dept, on=salted_emp.salted_dept_id==salted_dept.salted_dept_id, how=\"left_outer\")"
      ],
      "metadata": {
        "id": "mADHsRvUXIDy"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "salted_joined_df.write.format(\"noop\").mode(\"overwrite\").save()"
      ],
      "metadata": {
        "id": "Psm3fctNXIA2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the partition details to understand distribution\n",
        "from pyspark.sql.functions import spark_partition_id, count\n",
        "\n",
        "part_df = salted_joined_df.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\").agg(count(lit(1)).alias(\"count\"))\n",
        "\n",
        "part_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t05IRSBxXH-4",
        "outputId": "8432f2a9-964a-44ab-bd26-231e601353f2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------+\n",
            "|partition_num| count|\n",
            "+-------------+------+\n",
            "|           10|473632|\n",
            "+-------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wUSQd7ATXH83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8S2uKfQrXH5y"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}