{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "x_tNJdb38OfP",
        "outputId": "dbd1bb34-0ab3-4fab-cf66-b82bf0b4aa3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7a626ebcf920>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://b102aef896fc:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Dynamic Allocation</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .appName(\"Dynamic Allocation\")\n",
        "    .master(\"local[*]\")  # Use local mode for development\n",
        "    .config(\"spark.executor.cores\", 2)\n",
        "    .config(\"spark.executor.memory\", \"512M\")\n",
        "    .config(\"spark.dynamicAllocation.enabled\", True)\n",
        "    .config(\"spark.dynamicAllocation.minExecutors\", 0)\n",
        "    .config(\"spark.dynamicAllocation.maxExecutors\", 5)\n",
        "    .config(\"spark.dynamicAllocation.initialExecutors\", 1)\n",
        "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", True)\n",
        "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"60s\")\n",
        "    .config(\"spark.dynamicAllocation.cachedExecutorIdleTimeout\", \"60s\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1U_qoqQ8OfS"
      },
      "outputs": [],
      "source": [
        "# Read Sales data\n",
        "\n",
        "sales_schema = \"transacted_at string, trx_id string, retailer_id string, description string, amount double, city_id string\"\n",
        "\n",
        "sales = spark.read.format(\"csv\").schema(sales_schema).option(\"header\", True).load(r\"/content/emp_sales.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtLrpiH08OfS"
      },
      "outputs": [],
      "source": [
        "# Read City data\n",
        "\n",
        "city_schema = \"city_id string, city string, state string, state_abv string, country string\"\n",
        "\n",
        "city = spark.read.format(\"csv\").schema(city_schema).option(\"header\", True).load(r\"/content/cities.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1tyc1W98OfS"
      },
      "outputs": [],
      "source": [
        "# Join Data\n",
        "\n",
        "df_sales_joined = sales.join(city, on=sales.city_id==city.city_id, how=\"left_outer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDs8LbnJ8OfS"
      },
      "outputs": [],
      "source": [
        "df_sales_joined.write.format(\"noop\").mode(\"overwrite\").save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gh1zxey8OfT"
      },
      "outputs": [],
      "source": [
        "# Difference between Scale UP in Databricks and Dynamic Allocation"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}